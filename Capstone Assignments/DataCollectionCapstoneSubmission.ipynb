{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBHHouniGE85NBU+Gb0OCQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liamtabrams/UCSD-AI-Bootcamp/blob/main/Capstone%20Assignments/DataCollectionCapstoneSubmission.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "To view the csv datasets that are saved in Github, go to the folder called \"16.4-Data_Collection\" at the same level as this jupyter notebook."
      ],
      "metadata": {
        "id": "OWXc7aQYhQyJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "'''the following is all of the Python code needed to create and\n",
        "save the 15,670 row dataframe of Johnson & Johnson ticker history\n",
        "as a CSV file. This is essentially time series data'''\n",
        "!pip install yfinance\n",
        "\n",
        "import yfinance as yahooFinance\n",
        "\n",
        "GetJNJInformation = yahooFinance.Ticker(\"JNJ\")\n",
        "\n",
        "print(GetJNJInformation.history(period=\"max\").shape)\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame(GetJNJInformation.history(period=\"max\"))\n",
        "\n",
        "# Save DataFrame to a CSV file\n",
        "df.to_csv('JNJTickerHistory.csv', index=True)\n",
        "\n",
        "print(GetJNJInformation.history(period=\"max\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bw--v6N21Qur",
        "outputId": "9609b64f-0301-43f9-9e22-80bd9642447b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.37)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.0.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.25.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2023.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.1)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.2.2)\n",
            "(15671, 7)\n",
            "                                 Open        High         Low       Close  \\\n",
            "Date                                                                        \n",
            "1962-01-02 00:00:00-05:00    0.000000    0.064012    0.063680    0.064012   \n",
            "1962-01-03 00:00:00-05:00    0.000000    0.063349    0.063017    0.063017   \n",
            "1962-01-04 00:00:00-05:00    0.000000    0.063349    0.062354    0.062354   \n",
            "1962-01-05 00:00:00-05:00    0.000000    0.061856    0.061359    0.061359   \n",
            "1962-01-08 00:00:00-05:00    0.000000    0.060861    0.060364    0.060364   \n",
            "...                               ...         ...         ...         ...   \n",
            "2024-03-28 00:00:00-04:00  158.199997  159.139999  158.110001  158.190002   \n",
            "2024-04-01 00:00:00-04:00  157.720001  158.149994  156.770004  157.779999   \n",
            "2024-04-02 00:00:00-04:00  156.570007  157.830002  155.949997  157.729996   \n",
            "2024-04-03 00:00:00-04:00  157.789993  158.050003  154.250000  154.259995   \n",
            "2024-04-04 00:00:00-04:00  155.309998  155.500000  152.410004  152.500000   \n",
            "\n",
            "                            Volume  Dividends  Stock Splits  \n",
            "Date                                                         \n",
            "1962-01-02 00:00:00-05:00        0        0.0           0.0  \n",
            "1962-01-03 00:00:00-05:00   345600        0.0           0.0  \n",
            "1962-01-04 00:00:00-05:00   216000        0.0           0.0  \n",
            "1962-01-05 00:00:00-05:00   129600        0.0           0.0  \n",
            "1962-01-08 00:00:00-05:00   172800        0.0           0.0  \n",
            "...                            ...        ...           ...  \n",
            "2024-03-28 00:00:00-04:00  6181500        0.0           0.0  \n",
            "2024-04-01 00:00:00-04:00  4364700        0.0           0.0  \n",
            "2024-04-02 00:00:00-04:00  6102300        0.0           0.0  \n",
            "2024-04-03 00:00:00-04:00  8096000        0.0           0.0  \n",
            "2024-04-04 00:00:00-04:00  6889901        0.0           0.0  \n",
            "\n",
            "[15671 rows x 7 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''the following code is used to script all transcripts from SpongeBob Wikia and save to a csv file\n",
        "with character and line as columns'''\n",
        "\n",
        "#import libraries\n",
        "from bs4 import BeautifulSoup\n",
        "from urllib.request import urlopen\n",
        "import csv\n",
        "import re\n",
        "# url to look at\n",
        "transcripts = \"http://spongebob.wikia.com/wiki/List_of_transcripts#Season\"\n",
        "# opens url and reads html contents\n",
        "rawHTML = urlopen(transcripts).read()\n",
        "\n",
        "# makes beautiful soup object and parses html from opened url\n",
        "soup = BeautifulSoup(rawHTML, \"html.parser\")\n",
        "\n",
        "links = []\n",
        "\n",
        "#looks through each 'a' tag in the second table in the webpage\n",
        "for table in soup.find_all(\"table\"):\n",
        "    names = table.find_all(\"a\")\n",
        "    for name in names:\n",
        "    # strips string of whitespace (some have intitial spaces)\n",
        "    # and concatenates it with homepage url to form the full url\n",
        "    # adds it to the list of links\n",
        "        links += [\"http://spongebob.wikia.com\" + name.get(\"href\").strip()]\n",
        "\n",
        "#we end up with more than just the links so we need to remove that part of the list\n",
        "for i in range(868,len(links)):\n",
        "    links.pop()\n",
        "#we ended up with double the links so we need to get half of them, each duplicate comes directly after the first\n",
        "linksnew = []\n",
        "for i in range(867):\n",
        "    if i%2 == 1:\n",
        "        linksnew.append(links[i])\n",
        "#create empty list\n",
        "J = []\n",
        "for link in linksnew:\n",
        "    #open HTML file\n",
        "    firstHTML = urlopen(link).read()\n",
        "    #create beautiful soup object\n",
        "    html_soup = BeautifulSoup(firstHTML, 'html.parser')\n",
        "\n",
        "    # find the <ul> content, aka the actual script\n",
        "    content = html_soup.find('ul', attrs={'class': None})\n",
        "\n",
        "    # find each bullet\n",
        "    lines = content.findAll('li')\n",
        "    #create an empty list to store cleaned lines\n",
        "    P = []\n",
        "    for x in lines:\n",
        "        P.append(re.sub(\"[\\(\\[].*?[\\)\\]]\", \"\", x.getText()))\n",
        "    #create an empty list to store cleaned lines from P as strings\n",
        "    L = []\n",
        "    for i in P:\n",
        "        L.append(str(i))\n",
        "    #create empty list to hold lines after removing new line characters\n",
        "    M = []\n",
        "    for line in L:\n",
        "        m = line.strip('\\n')\n",
        "        M.append(m)\n",
        "    #split each element of M into a list of character and line, and add it to J\n",
        "    for line in M:\n",
        "        J.append(line.split(':'))\n",
        "\n",
        "#create temporary SpongeBob transcript file\n",
        "with open('temp_spongebob_transcript.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "        spamwriter = csv.writer(csvfile, delimiter=',')\n",
        "        for line in J:\n",
        "            spamwriter.writerow(line)\n",
        "\n",
        "#go into the temporary SpongeBob file and put any parts of lines that are past the 2nd column into the 2nd column in the new SpongeBob file\n",
        "input_file = open(\"temp_spongebob_transcript.csv\", \"r\", encoding=\"utf8\")\n",
        "with open('spongebob_transcript.csv', 'w', newline='', encoding='utf-8') as csvfile:\n",
        "    spamwriter = csv.writer(csvfile, delimiter=',')\n",
        "    for row in csv.reader(input_file):\n",
        "        if len(row)>0:\n",
        "            if len(row)>= 2:\n",
        "                line = row[1]\n",
        "            else:\n",
        "                line = ''\n",
        "            for i in range(2,len(row)):\n",
        "                line += row[i]\n",
        "            L = [row[0],line]\n",
        "            spamwriter.writerow(L)"
      ],
      "metadata": {
        "id": "T2oA6TqS3VJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For the sake of finding something readily available and largely clean rather than employ diligent scraping, I decided to download an Electric Vehicle Population Dataset at https://catalog.data.gov/dataset/electric-vehicle-population-data, in the form of a CSV file. It is actually quite interesting, and provides quite a bit of interesting info for members of the population with over 170,000 samples!"
      ],
      "metadata": {
        "id": "QdJmqOCMbaZS"
      }
    }
  ]
}